{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "PySpark",
      "language": "",
      "name": "pysparkkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "mimetype": "text/x-python",
      "name": "pyspark",
      "pygments_lexer": "python3"
    },
    "colab": {
      "name": "spark-emr.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aiLo3RmjtqO",
        "colab_type": "text"
      },
      "source": [
        "# Training MLLib model on EMR cluster\n",
        "\n",
        "## Creating an AWS key-pair\n",
        "\n",
        "To access the instances on AWS, we first need a key-pair. If you don’t have one, create one in https://us-east-2.console.aws.amazon.com/ec2/home?region=us-east-2#KeyPairs and save it to your computer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7Jti5rjjtqP",
        "colab_type": "text"
      },
      "source": [
        "# Setting up Spark cluster\n",
        "\n",
        "As a first step we’ll create a Spark cluster on AWS EMR (Elastic Map Reduce)\n",
        "\n",
        "\n",
        "1. Go to https://us-east-2.console.aws.amazon.com/elasticmapreduce and click on **Create cluster** and then **Go to Advanced options**\n",
        "\n",
        "2. Select latest EMR version and check Hadoop, Spark and Livy\n",
        "![](https://paper-attachments.dropbox.com/s_7E296B3828F47DAA4B2AC26953FEE16C616F9AA7984483B4ED1335DCB156EBC3_1587027516546_image.png)\n",
        "\n",
        "3. Remove **Task** node type and change Core instance type to **Spot** (you can use Spot also for Master, but you risk that it’ll be terminated). You may select smaller instances if you wish, but for this type of cluster (32GB memory) you’d pay ~$0.45/hour\n",
        "![](https://paper-attachments.dropbox.com/s_7E296B3828F47DAA4B2AC26953FEE16C616F9AA7984483B4ED1335DCB156EBC3_1587027664960_image.png)\n",
        "\n",
        "4. Name your cluster\n",
        "![](https://paper-attachments.dropbox.com/s_7E296B3828F47DAA4B2AC26953FEE16C616F9AA7984483B4ED1335DCB156EBC3_1587027700424_image.png)\n",
        "\n",
        "5. Use you key-pair\n",
        "![](https://paper-attachments.dropbox.com/s_7E296B3828F47DAA4B2AC26953FEE16C616F9AA7984483B4ED1335DCB156EBC3_1587029054377_image.png)\n",
        "\n",
        "6. After the cluster starts, you should see this\n",
        "![](https://paper-attachments.dropbox.com/s_7E296B3828F47DAA4B2AC26953FEE16C616F9AA7984483B4ED1335DCB156EBC3_1587029513107_image.png)\n",
        "\n",
        "\n",
        "7. AWS is by default blocking all requests to the cluster from outside, so to be able to SSH into it we need to open port 22 for EMR security group.\n",
        "    1. Go to https://us-east-2.console.aws.amazon.com/ec2/home?region=us-east-2#SecurityGroups\n",
        "    2. Find security group **ElasticMapReduce-master**\n",
        "    3. Click on **Inbound rules** below → **Edit Inbound rules**\n",
        "    4. Add SSH and allow it from Anywhere\n",
        "![](https://paper-attachments.dropbox.com/s_7E296B3828F47DAA4B2AC26953FEE16C616F9AA7984483B4ED1335DCB156EBC3_1587030515578_image.png)\n",
        "\n",
        "8. Now we have to establish connection through SSH forwarding. Click on **Enable Web Connection** and copy the SSH command (you need to change the path to your key-pair). It should look like this\n",
        "    ```\n",
        "    ssh -i ~/mojmir.pem -ND 8157 hadoop@ec2-3-15-234-116.us-east-2.compute.amazonaws.com\n",
        "    ```\n",
        "\n",
        "    If you did everything correctly, you can go to the following URL in your browser\n",
        "\n",
        "    ```\n",
        "    ec2-3-19-69-20.us-east-2.compute.amazonaws.com:8998\n",
        "    ```\n",
        "\n",
        "    and see Livy UI\n",
        "    \n",
        "![](https://paper-attachments.dropbox.com/s_7E296B3828F47DAA4B2AC26953FEE16C616F9AA7984483B4ED1335DCB156EBC3_1587045193456_image.png)\n",
        "\n",
        "9. The last step will be to run local port forwarding on localhost so that we can access the cluster from sparkmagic. The following only forwards port 8998 where Livy lives\n",
        "\n",
        "```\n",
        "ssh -i ~/mojmir.pem -N -L 8998:localhost:8998 hadoop@ec2-3-19-69-20.us-east-2.compute.amazonaws.com\n",
        "```\n",
        "\n",
        "10. If you are on Windows, either use Putty https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html or alternatively change IP in sparkmagic config\n",
        "\n",
        "  1. Download config file https://raw.githubusercontent.com/jupyter-incubator/sparkmagic/master/sparkmagic/example_config.json and rename it to config.json\n",
        "\n",
        "  2. Replace all occurences of http://localhost:8998 by Livy address (e.g. http://ec2-52-14-86-104.us-east-2.compute.amazonaws.com:8998)\n",
        "\n",
        "  3. Copy `config.json` to `[home directory]/.sparkmagic/config.json` (there should be an empty file already, so replace it)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6_PSds3jtqP",
        "colab_type": "text"
      },
      "source": [
        "# Connecting to Spark cluster\n",
        "\n",
        "Now that we have estabilished connection to our cluster, we’ll use [sparkmagic](https://github.com/jupyter-incubator/sparkmagic) to connect to it. To install sparkmagic\n",
        "\n",
        "1. `pip install sparkmagic`\n",
        "2. `jupyter nbextension enable --py --sys-prefix widgetsnbextension` \n",
        "3. (Only if you use Jupyterlab) `jupyter labextension install @jupyter-widgets/jupyterlab-manager`\n",
        "4. Check location of sparkmagic with `pip show sparkmagic`, then `cd` into that directory\n",
        "    ```\n",
        "    cd /usr/local/lib/python3.7/site-packages\n",
        "    ```\n",
        "    and install kernel\n",
        "    ```\n",
        "    jupyter-kernelspec install sparkmagic/kernels/pysparkkernel\n",
        "    ```\n",
        "\n",
        "5. Open Jupyter notebook / Lab and create PySpark notebook\n",
        "    \n",
        "![](https://paper-attachments.dropbox.com/s_7E296B3828F47DAA4B2AC26953FEE16C616F9AA7984483B4ED1335DCB156EBC3_1587045389624_image.png)\n",
        "\n",
        "6. Run `sc` in the cell. After a while you should see your new Spark Context!\n",
        "\n",
        "![](https://paper-attachments.dropbox.com/s_7E296B3828F47DAA4B2AC26953FEE16C616F9AA7984483B4ED1335DCB156EBC3_1587045471061_image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyenlFNMjtqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%configure -f\n",
        "{ \"conf\":{\n",
        "          \"spark.pyspark.python\": \"/usr/bin/python3\",\n",
        "          \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
        "          \"spark.pyspark.virtualenv.type\":\"native\",\n",
        "          \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
        "         }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE_cEi58jtqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm_kenvjjtqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list available packages in pyspark\n",
        "sc.list_packages()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFM9aqzOjtqY",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "1. Create new bucket `mlcollege` (use your own name as bucket names must be unique) in S3 (https://s3.console.aws.amazon.com/s3/home?region=eu-central-1)\n",
        "\n",
        "2. To simulate the real world situation, first upload adult data from https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data to that bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEqF74aojtqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [\n",
        "    'age',\n",
        "    'workclass', \n",
        "    'fnlwgt',\n",
        "    'education',\n",
        "    'education_num',\n",
        "    'marital_status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'race',\n",
        "    'sex',\n",
        "    'capital_gain',\n",
        "    'capital_loss',\n",
        "    'hours_per_week',\n",
        "    'native_country',\n",
        "    'income',\n",
        "]\n",
        "\n",
        "# read data from S3\n",
        "df = spark.read.csv(\"s3://mlcollege/adult.data\", header=False, inferSchema=True, mode='FAILFAST')\n",
        "df = df.toDF(*columns)\n",
        "df.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J7F8OOWjtqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# repartition it and save to parquet format\n",
        "df.repartition(5).write.parquet(\"s3n://mlcollege/adult.parquet\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_xpDOe0jtqd",
        "colab_type": "text"
      },
      "source": [
        "# Load training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOc0Ibkdjtqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = spark.read.parquet(\"s3n://mlcollege/adult.parquet\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcc_bXIsjtqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPPhRqnujtqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import functions\n",
        "\n",
        "# create label (note the extra whitespace)\n",
        "df = df.withColumn('label', (functions.ltrim(df.income) == '>50K').cast('int'))\n",
        "\n",
        "# persist dataframe in memory to avoid loading on every command\n",
        "df.persist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YdS3_cijtqk",
        "colab_type": "text"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j76iQqCKjtql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# create pipeline that will accept two features `hours_per_week` and `education_num`\n",
        "\n",
        "assembler = VectorAssembler(inputCols=['hours_per_week', 'education_num'], outputCol = 'features')\n",
        "regressor = LogisticRegression(featuresCol = 'features', labelCol = 'label')\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, regressor])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvoyxysVjtqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train model\n",
        "model = pipeline.fit(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rijljsyjtqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test that it works\n",
        "model.transform(df.select('hours_per_week', 'education_num')).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY1EV0BQjtqs",
        "colab_type": "text"
      },
      "source": [
        "# Save model with mlflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sukvxdt4jtqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc.install_pypi_package(\"mlflow\")\n",
        "sc.install_pypi_package(\"boto3\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYDyeCjkjtqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mlflow.spark\n",
        "\n",
        "version = 'v3'\n",
        "\n",
        "# need to save to /tmp because we don't have permissions to write anywhere else\n",
        "mlflow.spark.save_model(model, f\"/tmp/models/spark-adult-model-{version}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHeWYqI4jtqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "def make_tarfile(output_filename, source_dir):\n",
        "    \"\"\"Compress directory into tar.gz\"\"\"\n",
        "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
        "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
        "        \n",
        "make_tarfile(f'/tmp/models/spark-adult-model-{version}.tar.gz', f'/tmp/models/spark-adult-model-{version}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WAEhK7Jjtqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import boto3\n",
        "\n",
        "# upload to S3\n",
        "s3 = boto3.resource('s3')\n",
        "s3.meta.client.upload_file(f'/tmp/models/spark-adult-model-{version}.tar.gz', 'mlcollege', f'models/spark-adult-model-{version}.tar.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQhZTGK-jtq0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Model Serving\n",
        "1. Download the model from S3 and uncompress it\n",
        "2. Install mlflow with `pip install mlflow`\n",
        "3. Serve the model with `PYSPARK_PYTHON=python3 mlflow models serve -m spark-adult-model-v2 --no-conda`\n",
        "4. Start making predictions!\n",
        "```\n",
        "    curl -d '{\"columns\":[\"hours_per_week\", \"education_num\"], \"data\":[[100.0, 9.0], [40.0, 13.0]]}' -H 'Content-Type: application/json; format=pandas-split' -X POST localhost:5000/invocations\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPozJnYajtq3",
        "colab_type": "text"
      },
      "source": [
        "# Tasks\n",
        "\n",
        "1. Try to add non-numeric features to the model, retrain it and try making prediction\n",
        "\n",
        "2. Restart your cluster and try it with [Zeppelin notebook](https://zeppelin.apache.org/)\n",
        "    1. Instead of adding **Livy** to software configuration, add **Zeppelin**\n",
        "    2. Connect to it on port 8890\n",
        "    3. Zeppelin uses Scala by default, but you can use `%pyspark` at the beginning of every block to run PySpark\n",
        "    4. Retrain your model in the same way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nskUlhAFjtq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}